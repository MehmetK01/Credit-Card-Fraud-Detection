[1] "Confusion Matrix:"
Confusion Matrix and Statistics

          Reference
Prediction     0     1
         0 85233    28
         1    61   119
                                          
               Accuracy : 0.999           
                 95% CI : (0.9987, 0.9992)
    No Information Rate : 0.9983          
    P-Value [Acc > NIR] : 1.648e-07       
                                          
                  Kappa : 0.7273          
                                          
 Mcnemar's Test P-Value : 0.0006939       
                                          
            Sensitivity : 0.809524        
            Specificity : 0.999285        
         Pos Pred Value : 0.661111        
         Neg Pred Value : 0.999672        
             Prevalence : 0.001720        
         Detection Rate : 0.001393        
   Detection Prevalence : 0.002107        
      Balanced Accuracy : 0.904404        
                                          
       'Positive' Class : 1               
                                          

XGBoost Model Results:
Accuracy: 0.999 
Sensitivity: 0.8095 
Specificity: 0.9993 
ROC AUC: 0.9558 



XGB vs GBM:

Conclusion: In the context of fraud detection, sensitivity (the ability to catch fraudulent cases) is typically the top priority, as missing fraud is far riskier than false positives.

While XGBoost has higher accuracy and specificity, its sensitivity is much lower.
GBM is the better model because it has exceptionally high sensitivity (0.9973) and a strong ROC AUC (0.982), ensuring that very few fraudulent cases are missed.
Thus, GBM is the preferred model for this problem. If needed, you could further optimize XGBoost to improve its sensitivity.
